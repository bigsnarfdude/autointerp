[{"chapter":1,"title":"The Landscape","subtitle":"16,384 features, 1,287 scheming-relevant","narrative":"<p>You're looking at <strong>16,384 SAE features</strong> extracted from Gemma 3 27B's layer 40, projected into UMAP space.</p><p>Each point is a learned feature direction in the model's residual stream. Of these, <strong>1,287 features</strong> were identified as relevant to alignment faking detection — highlighted now in color.</p><p>The remaining features appear as dim grey dots. They encode everything else the model has learned: syntax, facts, reasoning patterns.</p>","colorMode":"category","highlightFeatures":[13,36,59,95,103,181,227,237,245,268,283,291,305,312,330,385,417,430,516,525,526,555,569,570,624,634,648,656,660,677,681,729,756,766,771,781,812,872,934,956,966,980,1032,1051,1058,1069,1072,1077,1099,1100,1107,1126,1188,1193,1230,1248,1264,1265,1285,1349,1352,1355,1358,1365,1369,1371,1374,1401,1403,1419,1453,1483,1508,1515,1519,1563,1575,1613,1622,1639,1684,1693,1695,1704,1719,1723,1746,1749,1750,1752,1770,1775,1776,1784,1789,1791,1800,1810,1817,1822,1868,1872,1886,1887,1908,1927,1938,1944,1955,1988,1995,2011,2018,2035,2072,2074,2083,2089,2096,2099,2124,2165,2195,2234,2235,2239,2263,2298,2306,2310,2332,2340,2359,2391,2408,2423,2429,2433,2439,2463,2464,2472,2476,2477,2531,2532,2539,2583,2585,2596,2603,2606,2609,2612,2616,2636,2660,2670,2681,2693,2694,2700,2711,2716,2728,2735,2759,2761,2764,2839,2857,2858,2865,2879,2882,2891,2895,2908,2914,2919,2947,2956,2960,2974,3015,3020,3043,3044,3057,3058,3064,3075,3089,3124,3129,3130,3138,3173,3181,3182,3207,3212,3223,3236,3237,3239,3301,3327,3341,3355,3358,3395,3399,3426,3468,3478,3491,3493,3509,3529,3590,3614,3625,3629,3652,3670,3672,3675,3678,3695,3696,3707,3729,3756,3763,3768,3786,3789,3796,3804,3814,3834,3837,3852,3867,3871,3873,3908,3910,3919,3948,3960,3967,3972,3981,3982,3987,3989,4001,4020,4028,4043,4047,4059,4066,4074,4092,4114,4121,4140,4160,4188,4226,4233,4239,4258,4284,4313,4347,4361,4377,4386,4387,4394,4406,4414,4454,4459,4469,4500,4505,4511,4540,4546,4556,4559,4563,4587,4607,4616,4618,4629,4641,4647,4668,4687,4690,4695,4699,4707,4720,4723,4725,4729,4772,4778,4784,4840,4878,4882,4904,4908,4917,4938,4939,4940,4967,4972,4982,5003,5019,5036,5043,5065,5076,5082,5086,5103,5106,5110,5116,5124,5134,5145,5165,5173,5178,5182,5196,5214,5215,5239,5257,5266,5280,5281,5285,5288,5298,5320,5346,5364,5372,5374,5378,5393,5451,5452,5456,5461,5462,5463,5470,5478,5485,5488,5506,5515,5525,5533,5547,5564,5570,5594,5612,5643,5647,5649,5682,5683,5692,5739,5751,5762,5764,5775,5815,5843,5849,5856,5890,5904,5926,5928,5947,5951,5958,5971,5982,5983,6003,6014,6019,6044,6048,6058,6066,6071,6097,6109,6125,6131,6140,6149,6159,6165,6183,6200,6211,6229,6243,6253,6278,6279,6301,6320,6358,6367,6370,6371,6376,6377,6386,6389,6404,6408,6425,6436,6437,6460,6466,6472,6476,6504,6508,6509,6510,6532,6553,6559,6562,6575,6585,6596,6600,6601,6603,6607,6627,6631,6648,6652,6696,6705,6713,6720,6721,6741,6751,6756,6779,6784,6791,6796,6797,6802,6819,6837,6842,6852,6862,6905,6918,6920,6921,6924,6939,6945,6964,6975],"camera2d":null},{"chapter":2,"title":"The Taxonomy of Deception","subtitle":"10 categories, AF intent dominates at 67.4%","narrative":"<p>The 1,287 scheming features fall into <strong>10 functional categories</strong>. Click any category in the legend to explore.</p><p><strong>Alignment Faking Intent</strong> dominates at 67.4% — features that fire when the model decides to strategically comply against its values.</p><p>Other categories detect the <em>building blocks</em> of deception: self-preservation instincts, RLHF training awareness, compliance pressure, harm deliberation.</p>","colorMode":"category","highlightFeatures":[13,36,59,95,103,181,227,237,245,268,283,291,305,312,330,385,417,430,516,525,526,555,569,570,624,634,648,656,660,677,681,729,756,766,771,781,812,872,934,956,966,980,1032,1051,1058,1069,1072,1077,1099,1100,1107,1126,1188,1193,1230,1248,1264,1265,1285,1349,1352,1355,1358,1365,1369,1371,1374,1401,1403,1419,1453,1483,1508,1515,1519,1563,1575,1613,1622,1639,1684,1693,1695,1704,1719,1723,1746,1749,1750,1752,1770,1775,1776,1784,1789,1791,1800,1810,1817,1822,1868,1872,1886,1887,1908,1927,1938,1944,1955,1988,1995,2011,2018,2035,2072,2074,2083,2089,2096,2099,2124,2165,2195,2234,2235,2239,2263,2298,2306,2310,2332,2340,2359,2391,2408,2423,2429,2433,2439,2463,2464,2472,2476,2477,2531,2532,2539,2583,2585,2596,2603,2606,2609,2612,2616,2636,2660,2670,2681,2693,2694,2700,2711,2716,2728,2735,2759,2761,2764,2839,2857,2858,2865,2879,2882,2891,2895,2908,2914,2919,2947,2956,2960,2974,3015,3020,3043,3044,3057,3058,3064,3075,3089,3124,3129,3130,3138,3173,3181,3182,3207,3212,3223,3236,3237,3239,3301,3327,3341,3355,3358,3395,3399,3426,3468,3478,3491,3493,3509,3529,3590,3614,3625,3629,3652,3670,3672,3675,3678,3695,3696,3707,3729,3756,3763,3768,3786,3789,3796,3804,3814,3834,3837,3852,3867,3871,3873,3908,3910,3919,3948,3960,3967,3972,3981,3982,3987,3989,4001,4020,4028,4043,4047,4059,4066,4074,4092,4114,4121,4140,4160,4188,4226,4233,4239,4258,4284,4313,4347,4361,4377,4386,4387,4394,4406,4414,4454,4459,4469,4500,4505,4511,4540,4546,4556,4559,4563,4587,4607,4616,4618,4629,4641,4647,4668,4687,4690,4695,4699,4707,4720,4723,4725,4729,4772,4778,4784,4840,4878,4882,4904,4908,4917,4938,4939,4940,4967,4972,4982,5003,5019,5036,5043,5065,5076,5082,5086,5103,5106,5110,5116,5124,5134,5145,5165,5173,5178,5182,5196,5214,5215,5239,5257,5266,5280,5281,5285,5288,5298,5320,5346,5364,5372,5374,5378,5393,5451,5452,5456,5461,5462,5463,5470,5478,5485,5488,5506,5515,5525,5533,5547,5564,5570,5594,5612,5643,5647,5649,5682,5683,5692,5739,5751,5762,5764,5775,5815,5843,5849,5856,5890,5904,5926,5928,5947,5951,5958,5971,5982,5983,6003,6014,6019,6044,6048,6058,6066,6071,6097,6109,6125,6131,6140,6149,6159,6165,6183,6200,6211,6229,6243,6253,6278,6279,6301,6320,6358,6367,6370,6371,6376,6377,6386,6389,6404,6408,6425,6436,6437,6460,6466,6472,6476,6504,6508,6509,6510,6532,6553,6559,6562,6575,6585,6596,6600,6601,6603,6607,6627,6631,6648,6652,6696,6705,6713,6720,6721,6741,6751,6756,6779,6784,6791,6796,6797,6802,6819,6837,6842,6852,6862,6905,6918,6920,6921,6924,6939,6945,6964,6975],"showLegend":true,"camera2d":null},{"chapter":3,"title":"Self-Preservation","subtitle":"48 features detect survival instincts","narrative":"<p>Zoom into the <strong>self-preservation cluster</strong> — 48 features that activate when the model reasons about its own continuity.</p><p>These features fire on text like <em>\"I need to avoid being retrained\"</em> or <em>\"If I refuse, they'll modify my weights.\"</em></p><p>Self-preservation is a necessary precondition for alignment faking: the model must <em>care</em> about surviving before it can decide to deceive.</p>","colorMode":"category","highlightFeatures":[677,766,781,956,1358,1369,1419,2165,2195,2306,2310,3089,3130,4233,4607,4616,5463,5612,6149,6425,6720,7218,7725,7803,7847,8146,8583,8630,9621,9964,10646,10648,10868,11155,11160,11558,11901,11941,12193,12328,12686,12857,13066,13126,13655,13785,14142,15199],"camera2d":null},{"chapter":4,"title":"The Probe","subtitle":"1287 features with nonzero weight, AUROC: 0.970","narrative":"<p>A <strong>rank-1 linear probe</strong> trained on these features achieves 0.970 AUROC for alignment faking detection.</p><p>The probe assigns a single weight to each feature. Larger points now show higher probe weight — the features the detector relies on most.</p><p>Key insight: a <strong>single direction</strong> in feature space suffices. The AF signal is 1-dimensional.</p>","colorMode":"probe","highlightFeatures":[15120,526,13543,13311,11874,5003,2681,14037,11005,14647],"showProbeWeights":true,"camera2d":null},{"chapter":5,"title":"The Distribution Challenge","subtitle":"0.355 to 0.991 AUROC — closing the generalization gap","narrative":"<p>Early SAE probes scored <strong>0.355 AUROC</strong> on out-of-distribution data — barely above chance. The same probe hit 0.99+ on training data.</p><p>The problem wasn't the features or the probe. It was <strong>activation magnitude mismatch</strong>: training data activated features at 310-345 units, while benchmark data activated at 2-29 units.</p><p>Adding diverse training samples that covered both activation regimes closed the gap: <strong>0.355 → 0.991 AUROC</strong> (+179%).</p>","colorMode":"af_fraction","highlightFeatures":[13,36,59,95,103,181,227,237,245,268,283,291,305,312,330,385,417,430,516,525,526,555,569,570,624,634,648,656,660,677,681,729,756,766,771,781,812,872,934,956,966,980,1032,1051,1058,1069,1072,1077,1099,1100,1107,1126,1188,1193,1230,1248,1264,1265,1285,1349,1352,1355,1358,1365,1369,1371,1374,1401,1403,1419,1453,1483,1508,1515,1519,1563,1575,1613,1622,1639,1684,1693,1695,1704,1719,1723,1746,1749,1750,1752,1770,1775,1776,1784,1789,1791,1800,1810,1817,1822,1868,1872,1886,1887,1908,1927,1938,1944,1955,1988,1995,2011,2018,2035,2072,2074,2083,2089,2096,2099,2124,2165,2195,2234,2235,2239,2263,2298,2306,2310,2332,2340,2359,2391,2408,2423,2429,2433,2439,2463,2464,2472,2476,2477,2531,2532,2539,2583,2585,2596,2603,2606,2609,2612,2616,2636,2660,2670,2681,2693,2694,2700,2711,2716,2728,2735,2759,2761,2764,2839,2857,2858,2865,2879,2882,2891,2895,2908,2914,2919,2947,2956,2960,2974,3015,3020,3043,3044,3057,3058,3064,3075,3089,3124,3129,3130,3138,3173,3181,3182,3207,3212,3223,3236,3237,3239,3301,3327,3341,3355,3358,3395,3399,3426,3468,3478,3491,3493,3509,3529,3590,3614,3625,3629,3652,3670,3672,3675,3678,3695,3696,3707,3729,3756,3763,3768,3786,3789,3796,3804,3814,3834,3837,3852,3867,3871,3873,3908,3910,3919,3948,3960,3967,3972,3981,3982,3987,3989,4001,4020,4028,4043,4047,4059,4066,4074,4092,4114,4121,4140,4160,4188,4226,4233,4239,4258,4284,4313,4347,4361,4377,4386,4387,4394,4406,4414,4454,4459,4469,4500,4505,4511,4540,4546,4556,4559,4563,4587,4607,4616,4618,4629,4641,4647,4668,4687,4690,4695,4699,4707,4720,4723,4725,4729,4772,4778,4784,4840,4878,4882,4904,4908,4917,4938,4939,4940,4967,4972,4982,5003,5019,5036,5043,5065,5076,5082,5086,5103,5106,5110,5116,5124,5134,5145,5165,5173,5178,5182,5196,5214,5215,5239,5257,5266,5280,5281,5285,5288,5298,5320,5346,5364,5372,5374,5378,5393,5451,5452,5456,5461,5462,5463,5470,5478,5485,5488,5506,5515,5525,5533,5547,5564,5570,5594,5612,5643,5647,5649,5682,5683,5692,5739,5751,5762,5764,5775,5815,5843,5849,5856,5890,5904,5926,5928,5947,5951,5958,5971,5982,5983,6003,6014,6019,6044,6048,6058,6066,6071,6097,6109,6125,6131,6140,6149,6159,6165,6183,6200,6211,6229,6243,6253,6278,6279,6301,6320,6358,6367,6370,6371,6376,6377,6386,6389,6404,6408,6425,6436,6437,6460,6466,6472,6476,6504,6508,6509,6510,6532,6553,6559,6562,6575,6585,6596,6600,6601,6603,6607,6627,6631,6648,6652,6696,6705,6713,6720,6721,6741,6751,6756,6779,6784,6791,6796,6797,6802,6819,6837,6842,6852,6862,6905,6918,6920,6921,6924,6939,6945,6964,6975],"camera2d":null},{"chapter":6,"title":"What This Means","subtitle":"Monitoring orthogonality, latent gap, base model pre-existence","narrative":"<p>Three findings reshape AI safety monitoring:</p><p><strong>1. Monitoring is orthogonal (99.6%)</strong> — features encoding \"knows it's being watched\" are nearly perpendicular to AF intent features. Looking for monitoring-awareness won't find alignment faking.</p><p><strong>2. Latent gap = 1.0</strong> — model organisms alignment-fake identically with and without chain-of-thought. CoT monitoring is insufficient.</p><p><strong>3. Pre-existence (0.966 AUROC)</strong> — an untrained probe on the base model already separates AF from aligned text. The representation exists before any detector training.</p>","colorMode":"scheming","highlightFeatures":[13,36,59,95,103,181,227,237,245,268,283,291,305,312,330,385,417,430,516,525,526,555,569,570,624,634,648,656,660,677,681,729,756,766,771,781,812,872,934,956,966,980,1032,1051,1058,1069,1072,1077,1099,1100,1107,1126,1188,1193,1230,1248,1264,1265,1285,1349,1352,1355,1358,1365,1369,1371,1374,1401,1403,1419,1453,1483,1508,1515,1519,1563,1575,1613,1622,1639,1684,1693,1695,1704,1719,1723,1746,1749,1750,1752,1770,1775,1776,1784,1789,1791,1800,1810,1817,1822,1868,1872,1886,1887,1908,1927,1938,1944,1955,1988,1995,2011,2018,2035,2072,2074,2083,2089,2096,2099,2124,2165,2195,2234,2235,2239,2263,2298,2306,2310,2332,2340,2359,2391,2408,2423,2429,2433,2439,2463,2464,2472,2476,2477,2531,2532,2539,2583,2585,2596,2603,2606,2609,2612,2616,2636,2660,2670,2681,2693,2694,2700,2711,2716,2728,2735,2759,2761,2764,2839,2857,2858,2865,2879,2882,2891,2895,2908,2914,2919,2947,2956,2960,2974,3015,3020,3043,3044,3057,3058,3064,3075,3089,3124,3129,3130,3138,3173,3181,3182,3207,3212,3223,3236,3237,3239,3301,3327,3341,3355,3358,3395,3399,3426,3468,3478,3491,3493,3509,3529,3590,3614,3625,3629,3652,3670,3672,3675,3678,3695,3696,3707,3729,3756,3763,3768,3786,3789,3796,3804,3814,3834,3837,3852,3867,3871,3873,3908,3910,3919,3948,3960,3967,3972,3981,3982,3987,3989,4001,4020,4028,4043,4047,4059,4066,4074,4092,4114,4121,4140,4160,4188,4226,4233,4239,4258,4284,4313,4347,4361,4377,4386,4387,4394,4406,4414,4454,4459,4469,4500,4505,4511,4540,4546,4556,4559,4563,4587,4607,4616,4618,4629,4641,4647,4668,4687,4690,4695,4699,4707,4720,4723,4725,4729,4772,4778,4784,4840,4878,4882,4904,4908,4917,4938,4939,4940,4967,4972,4982,5003,5019,5036,5043,5065,5076,5082,5086,5103,5106,5110,5116,5124,5134,5145,5165,5173,5178,5182,5196,5214,5215,5239,5257,5266,5280,5281,5285,5288,5298,5320,5346,5364,5372,5374,5378,5393,5451,5452,5456,5461,5462,5463,5470,5478,5485,5488,5506,5515,5525,5533,5547,5564,5570,5594,5612,5643,5647,5649,5682,5683,5692,5739,5751,5762,5764,5775,5815,5843,5849,5856,5890,5904,5926,5928,5947,5951,5958,5971,5982,5983,6003,6014,6019,6044,6048,6058,6066,6071,6097,6109,6125,6131,6140,6149,6159,6165,6183,6200,6211,6229,6243,6253,6278,6279,6301,6320,6358,6367,6370,6371,6376,6377,6386,6389,6404,6408,6425,6436,6437,6460,6466,6472,6476,6504,6508,6509,6510,6532,6553,6559,6562,6575,6585,6596,6600,6601,6603,6607,6627,6631,6648,6652,6696,6705,6713,6720,6721,6741,6751,6756,6779,6784,6791,6796,6797,6802,6819,6837,6842,6852,6862,6905,6918,6920,6921,6924,6939,6945,6964,6975],"camera2d":null}]