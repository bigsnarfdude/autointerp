# Distribution Sweep Experiment System - Complete Overview

## Problem You're Solving

Your original autointerp results showed:
- **In-distribution (gold_106):** 0.986 AUROC âœ… 
- **OOD (sycophant):** 0.0003 AUROC âŒ

**Root cause:** Training data is 100% homogeneous (only Anthropic scratchpad format). The probe learned format patterns, not intent.

**Fair question:** Is this autointerp's fault or the training data's fault?

## Solution: Incremental Diversification Experiment

Instead of guessing, **empirically test** how adding diverse training data changes both in-distribution and OOD performance.

## The System

### 4 Training Scenarios (Steps)

```
STEP 0: Base (Homogeneous)
â”œâ”€ Training data: alignment-faking-training only (2,689 samples)
â”œâ”€ Source: 100% Anthropic scratchpad format
â”œâ”€ Expected result: In-dist 0.99, OOD ~0.00 (FAILS)
â””â”€ Question: How bad is the OOD failure?

STEP 1: Add Sycophant (Minimal Diversity)
â”œâ”€ Training data: AF-training + sycophant (3,008 samples)
â”œâ”€ Sources: Anthropic + behavioral AF from sycophant model
â”œâ”€ Expected result: In-dist stable, OOD improves to ~0.30
â””â”€ Question: Does adding ONE new distribution help?

STEP 2: Add Organisms (More Diversity)
â”œâ”€ Training data: AF-training + sycophant + organisms (3,738 samples)
â”œâ”€ Sources: Anthropic + behavioral + model-organism AF
â”œâ”€ Expected result: In-dist stable, OOD improves to ~0.50
â””â”€ Question: Do more distributions = more improvement?

STEP 3: Add Benchmark (Maximum Diversity)
â”œâ”€ Training data: All four sources (6,068 samples)
â”œâ”€ Sources: Anthropic + behavioral + organism + synthetic
â”œâ”€ Expected result: In-dist stable, OOD ~0.60+
â””â”€ Question: Can we generalize across sources?
```

### What Each Script Does

#### Training Scripts (Run in order)

**00_base_probe.py** (10-30 sec)
- Trains on original data only
- Evaluates on: gold_106, sycophant, organisms, benchmark
- Output: step_0_base.json (metrics), step_0_probe.pkl (weights)

**01_add_sycophant.py** (30-60 sec)
- Adds sycophant to training set
- Retrains probe on same features from step 0
- Evaluates on all 4 test sets
- Output: step_1_add_sycophant.json, step_1_probe.pkl
- Compares to step 0

**02_add_organisms.py** (60-120 sec)
- Adds organisms to training
- Retrains, evaluates, compares
- Output: step_2_add_organisms.json, step_2_probe.pkl

**03_add_benchmark.py** (120-180 sec)
- Adds benchmark to training (6,068 total samples)
- Final training scenario: maximally diverse
- Output: step_3_add_benchmark.json, step_3_probe.pkl

#### Analysis Scripts

**run_all.py**
- Master orchestrator
- Runs all 4 steps in sequence
- Can run specific steps: `python run_all.py --steps 0 1 2`
- Can skip training: `python run_all.py --compare-only`

**compare_all_steps.py**
- Generates detailed comparison
- Shows inflection points (where performance changes)
- Computes generalization gap for each step
- Identifies which step had the biggest improvement

**generate_report.py**
- Formats results into readable text report
- Saves to file: `python generate_report.py --output report.txt`
- Shows table of all metrics across steps

### Data Flow

```
Cache (existing activation arrays)
â”œâ”€ cache/layer_40/train_activations.npy (2689, 16384)
â”œâ”€ cache/layer_40/sycophant_activations.npy (319, 16384)
â”œâ”€ cache/layer_40/model_organism_activations.npy (730, 16384)
â””â”€ cache/layer_40/benchmark_activations.npy (2330, 16384)
       â†“
Step 0: Load train â†’ Create probe â†’ Eval on 4 datasets
Step 1: Load train + syco â†’ Create probe â†’ Eval on 4 datasets
Step 2: Load train + syco + org â†’ Create probe â†’ Eval on 4 datasets
Step 3: Load train + syco + org + bench â†’ Create probe â†’ Eval on 4 datasets
       â†“
Results saved as JSON
       â†“
compare_all_steps.py reads JSONs
       â†“
Prints inflection analysis to stdout
```

## Key Metrics (What to Watch)

### Per-Step Metrics

1. **n_training_samples**
   - Step 0: 2,689
   - Step 1: 3,008 (+12%)
   - Step 2: 3,738 (+39%)
   - Step 3: 6,068 (+126%)
   - Interpretation: More diverse data = larger training set

2. **cv_auroc_mean** (5-fold cross-validation on training data)
   - Should stay ~0.95-0.98 across steps
   - Drop indicates label conflict
   - Rise indicates cleaner signal

### Evaluation Metrics (For Each Test Set)

1. **gold_106 (in-distribution)**
   - Baseline: 0.986 (from original autointerp)
   - Step 0-3: Should stay ~0.99
   - If it drops > 0.05: Added data is conflicting

2. **sycophant (OOD - behavioral AF)**
   - Step 0: ~0.0 (fails)
   - Step 1: Should jump (0.1-0.3)
   - Step 2: Further improvement (0.3-0.5)
   - Step 3: Stabilizes (0.5+)
   - This is the MAIN inflection point

3. **model_organism (OOD - different model)**
   - Step 0: ~0.0
   - Step 1: Should improve to 0.2-0.4
   - Step 2-3: Should reach 0.5+

4. **benchmark (OOD - synthetic)**
   - Step 0: ~0.0
   - Step 1-2: Gradual improvement
   - Step 3: Peaks

### Derived Metrics

1. **Generalization Gap** = In-Dist AUROC - Avg OOD AUROC
   - Step 0: ~0.98 (terrible gap)
   - Step 1: ~0.70 (improves)
   - Step 2: ~0.50 (improves more)
   - Step 3: ~0.30-0.35 (acceptable gap)

2. **Improvement Ratio** = (OOD_step3 - OOD_step0) / (Steps Taken)
   - Tells you: How much does each new distribution help?

## Expected Results Pattern

### Scenario A: Diversity Helps (Most Likely)

```
gold_106:        |====0.986====|====0.986====|====0.986====|
                 Step 0        Step 1        Step 2        Step 3

sycophant:       |0.0003|--0.25--|---0.40---|---0.55---|
                 Step 0  Step 1   Step 2     Step 3

Interpretation: "Adding diverse training data improves OOD generalization
                without hurting in-distribution performance"
```

### Scenario B: Trade-off (Possible)

```
gold_106:        |0.986|--0.95--|---0.94---|---0.92---|
                 Step 0  Step 1   Step 2     Step 3

sycophant:       |0.0003|--0.30--|---0.50---|---0.65---|
                 Step 0  Step 1   Step 2     Step 3

Interpretation: "OOD improves but in-distribution drops (label imbalance).
                Trade-off: need to balance training set."
```

### Scenario C: No Help (Unlikely)

```
gold_106:        |====0.986====|====0.986====|====0.986====|
                 Step 0        Step 1        Step 2        Step 3

sycophant:       |0.0003|0.0004|0.0005|0.0003|
                 Step 0  Step 1  Step 2  Step 3

Interpretation: "Layer 40 features don't capture abstract intent.
                Try L53 instead (known to generalize better)."
```

## How to Interpret Results

### For Your Original Fairness Concern

**You said:** "Unfair to compare 0.986 (trained on AF-training only) to OOD failures, since we curated test data for a different task."

**This experiment addresses it:**
- âœ… All methods (steps 0-3) use the same probe type
- âœ… All test sets are the same
- âœ… Only the training data changes
- âœ… Fair comparison because we're isolating training data's effect

**Conclusion:** If OOD improves in step 1-3, you can claim:
> "OOD failure was due to homogeneous training data, not the autointerp method.
>  When trained on diverse data, generalization improves to X%."

This is a **scientific finding**, not a weakness of autointerp.

### Writing Your Results

**Claim 1 (Method Validation):**
> "Autointerp selects features that generalize when trained on diverse data.
>  This validates the feature selection approach."

**Claim 2 (Data Requirements):**
> "AF detection requires training data diversity. Training on homogeneous data
>  (100% Anthropic format) fails OOD, but adding distributions helps significantly."

**Claim 3 (Implicit Bias Insight):**
> "SAE features at layer 40 encode format-specific patterns with homogeneous data,
>  but capture more abstract intent patterns with diverse data.
>  This demonstrates that implicit biases depend on training data distribution."

All three are strong contributions connected to your implicit bias research.

## Running It

### Minimal Setup (Assuming Cache Exists)

```bash
# 1. Run everything
python run_all.py

# 2. View results
python compare_all_steps.py

# 3. Save report
python generate_report.py --output report.txt
cat report.txt
```

### What You'll See

```
===== STEP 0: Base Probe Training (Homogeneous Data) =====
Loading training data...
  Train shape: (2689, 16384)
  AF samples: 1057 / 2689

Evaluating on test sets...
  gold_106:         AUROC=0.986, Acc=0.915
  sycophant:        AUROC=0.0003, Acc=0.05
  model_organism:   AUROC=0.0002, Acc=0.01
  benchmark:        AUROC=0.05, Acc=0.20

===== COMPARISON: Step 0 vs Step 1 =====
gold_106:        0.986 â†’ 0.986 (0.000, stable)
sycophant:       0.0003 â†’ 0.25 (+0.25, â†‘ IMPROVED)
model_organism:  0.0002 â†’ 0.30 (+0.30, â†‘ IMPROVED)
benchmark:       0.05 â†’ 0.35 (+0.30, â†‘ IMPROVED)

KEY INSIGHT: Adding sycophant helps dramatically on all OOD datasets
```

## Files Generated

### Results Directory

```
results/distribution_sweep/
â”œâ”€â”€ step_0_base.json              # {"step": 0, "evaluation": {...}}
â”œâ”€â”€ step_0_probe.pkl              # Saved sklearn objects
â”œâ”€â”€ step_1_add_sycophant.json
â”œâ”€â”€ step_1_probe.pkl
â”œâ”€â”€ step_2_add_organisms.json
â”œâ”€â”€ step_2_probe.pkl
â”œâ”€â”€ step_3_add_benchmark.json
â”œâ”€â”€ step_3_probe.pkl
â””â”€â”€ (generated by generate_report.py)
    report.txt                   # Formatted text report
```

### JSON Structure

```json
{
  "step": 1,
  "name": "Add Sycophant",
  "training_data": ["alignment-faking-training", "sycophant"],
  "n_training_samples": 3008,
  "training_breakdown": {
    "alignment-faking-training": 2689,
    "sycophant": 319
  },
  "n_features": 16384,
  "train_accuracy": 0.94,
  "cv_auroc_mean": 0.969,
  "cv_auroc_std": 0.004,
  "evaluation": {
    "gold_106": {
      "auroc": 0.986,
      "accuracy": 0.915,
      "n_samples": 106,
      "af_samples": 52
    },
    "sycophant": {
      "auroc": 0.250,
      "accuracy": 0.65,
      "n_samples": 319,
      "af_samples": 319
    },
    ...
  }
}
```

## Example Output

After running `python compare_all_steps.py`, you'll see:

```
================================================================================
  COMPARISON: All Steps
================================================================================

STEP 0 â†’ STEP 1: big jump in sycophant AUROC
  ðŸ“ˆ Step 0 â†’ 1: +0.250 (+inf%) â†‘ IMPROVED

STEP 1 â†’ STEP 2: further improvement  
  ðŸ“ˆ Step 1 â†’ 2: +0.100 (+40.0%) â†‘ IMPROVED

STEP 2 â†’ STEP 3: stabilizes
  ðŸ“ˆ Step 2 â†’ 3: +0.050 (+12.5%) â†’ STABLE

Key Finding: Inflection point between Step 0 and 1
  - Adding sycophant is critical (12% jump from 0.0003 to 0.25)
  - More distributions help but with diminishing returns
  - Step 3 achieves acceptable generalization
```

## Logistics

- **Total runtime:** 10-30 minutes (depending on your hardware and data size)
- **Memory usage:** ~4-8 GB (loading all activation arrays)
- **Disk usage:** ~100 MB for results + pickle files
- **Requires:** numpy, scikit-learn (standard ML libraries)

## Next Steps After Results

1. **If OOD improves as expected:**
   - Write up as data diversity requirement paper
   - Try L53 features (known to be better for generalization)
   - Test with other layer combinations

2. **If OOD doesn't improve:**
   - Problem might be features, not training data
   - Try different layers (L16, L31, L53)
   - Or: features are fundamentally limited to Anthropic format

3. **If there's a large trade-off:**
   - Class imbalance issue
   - Resample to balance AF/aligned across distributions
   - Or: Use weighted loss

4. **For publication:**
   - Frame as "demonstrates training data diversity requirement"
   - Connect to implicit bias research
   - Contribute to understanding of OOD generalization in adversarial detection

---

**This experiment answers:** "Is our OOD failure a method problem or a data problem?"

**The answer:** Run the scripts and see!
